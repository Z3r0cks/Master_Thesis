@inproceedings{schleise_automated_2024,
	title = {An automated Pipeline to bring {NeRFs} to the Industrial Metaverse},
	url = {http://wscg.zcu.cz/WSCG2024/CSRN-2024/B59-2024.pdf},
	doi = {10.24132/CSRN.3401.17},
	abstract = {The Industrial Metaverse offers new opportunities in various industrial sectors, such as training, product development, and collaboration. It relies on rendering 3D images of industrial plants, factories, and machines that are viewed by multiple human observers in various applications. Creating these 3D scenes requires either elaborate 3D modeling efforts or complex 3D reconstruction methods. Both methods require expensive hardware and highly trained individuals to create photo-realistic results. To overcome these limitations, we offer an end-to-end pipeline that generates photo-realistic 3D scene renderings from video input using {NeRF} (neural radiance fields). Our tool automates the entire process, resulting in remarkable efficiency gains. It reduces the creation time from days to minutes, even for untrained users.},
	eventtitle = {Computer Science Research Notes},
	author = {Schleise, Sabine and Hahne, Uwe and {Hochschule Furtwangen} and Lindinger, Jakob and {SICK AG}},
	urldate = {2024-09-26},
	date = {2024},
}

@article{arribas_indirect_2020,
	title = {An indirect measurement of the speed of light in a General Physics Laboratory},
	volume = {32},
	issn = {10183647},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S101836472030210X},
	doi = {10.1016/j.jksus.2020.06.017},
	pages = {2797--2802},
	number = {6},
	journaltitle = {Journal of King Saud University - Science},
	shortjournal = {Journal of King Saud University - Science},
	author = {Arribas, Enrique and Escobar, Isabel and Ramirez-Vazquez, Raquel and Franco, Teresa and Belendez, Augusto},
	urldate = {2024-09-27},
	date = {2020-09},
	langid = {english},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2024-09-02},
	date = {2015-05-28},
	langid = {english},
}

@inproceedings{dehong_design_2017,
	location = {Chongqing, China},
	title = {Design and implementation of {LiDAR} navigation system based on triangulation measurement},
	url = {http://ieeexplore.ieee.org/document/7978258/},
	doi = {10.1109/ccdc.2017.7978258},
	eventtitle = {2017 29th Chinese Control And Decision Conference ({CCDC})},
	pages = {6060--6063},
	booktitle = {2017 29th Chinese Control And Decision Conference ({CCDC})},
	publisher = {{IEEE}},
	author = {Dehong, Cong and Liangqi, Zhang and Pengpeng, Su and Zaiyang, Tang and Yuhao, Meng and Yong, Wang},
	urldate = {2024-09-17},
	date = {2017-05},
}

@misc{niemeyer_differentiable_2020,
	title = {Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision},
	url = {http://arxiv.org/abs/1912.07372},
	shorttitle = {Differentiable Volumetric Rendering},
	abstract = {Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from {RGB} images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from {RGB} images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.},
	number = {{arXiv}:1912.07372},
	publisher = {{arXiv}},
	author = {Niemeyer, Michael and Mescheder, Lars and Oechsle, Michael and Geiger, Andreas},
	urldate = {2024-09-04},
	date = {2020-03-23},
	eprinttype = {arxiv},
	eprint = {1912.07372 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{muller_instant_2022,
	title = {Instant neural graphics primitives with a multiresolution hash encoding},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530127},
	doi = {10.1145/3528223.3530127},
	abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern {GPUs}. We leverage this parallelism by implementing the whole system using fully-fused {CUDA} kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
	pages = {1--15},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	urldate = {2024-09-18},
	date = {2022-07},
	langid = {english},
}

@misc{barron_mip-nerf_2021,
	title = {Mip-{NeRF}: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2103.13415},
	doi = {10.48550/ARXIV.2103.13415},
	shorttitle = {Mip-{NeRF}},
	abstract = {The rendering procedure used by neural radiance fields ({NeRF}) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for {NeRF}, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-{NeRF}" (a la "mipmap"), extends {NeRF} to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-{NeRF} reduces objectionable aliasing artifacts and significantly improves {NeRF}'s ability to represent fine details, while also being 7\% faster than {NeRF} and half the size. Compared to {NeRF}, mip-{NeRF} reduces average error rates by 17\% on the dataset presented with {NeRF} and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-{NeRF} is also able to match the accuracy of a brute-force supersampled {NeRF} on our multiscale dataset while being 22x faster.},
	publisher = {{arXiv}},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
	urldate = {2024-09-18},
	date = {2021},
	note = {Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Graphics (cs.{GR})},
}

@misc{zhang_nerf-lidar_2024,
	title = {{NeRF}-{LiDAR}: Generating Realistic {LiDAR} Point Clouds with Neural Radiance Fields},
	url = {http://arxiv.org/abs/2304.14811},
	shorttitle = {{NeRF}-{LiDAR}},
	abstract = {Labeling {LiDAR} point clouds for training autonomous driving is extremely expensive and difficult. {LiDAR} simulation aims at generating realistic {LiDAR} data with labels for training and verifying self-driving algorithms more efficiently. Recently, Neural Radiance Fields ({NeRF}) have been proposed for novel view synthesis using implicit reconstruction of 3D scenes. Inspired by this, we present {NeRF}-{LIDAR}, a novel {LiDAR} simulation method that leverages real-world information to generate realistic {LIDAR} point clouds. Different from existing {LiDAR} simulators, we use real images and point cloud data collected by self-driving cars to learn the 3D scene representation, point cloud generation and label rendering. We verify the effectiveness of our {NeRF}-{LiDAR} by training different 3D segmentation models on the generated {LiDAR} point clouds. It reveals that the trained models are able to achieve similar accuracy when compared with the same model trained on the real {LiDAR} data. Besides, the generated data is capable of boosting the accuracy through pre-training which helps reduce the requirements of the real labeled data.},
	number = {{arXiv}:2304.14811},
	publisher = {{arXiv}},
	author = {Zhang, Junge and Zhang, Feihu and Kuang, Shaochen and Zhang, Li},
	urldate = {2024-09-19},
	date = {2024-01-20},
	eprinttype = {arxiv},
	eprint = {2304.14811 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{mildenhall_nerf_2020,
	title = {{NeRF}: Representing Scenes as Neural Radiance Fields for View Synthesis},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2003.08934},
	doi = {10.48550/ARXIV.2003.08934},
	shorttitle = {{NeRF}},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(θ, ϕ)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	publisher = {{arXiv}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	urldate = {2024-07-18},
	date = {2020},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Graphics (cs.{GR})},
}

@article{tancik_nerfstudio_2023,
	title = {Nerfstudio: A Modular Framework for Neural Radiance Field Development},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2302.04264},
	doi = {10.48550/ARXIV.2302.04264},
	shorttitle = {Nerfstudio},
	abstract = {Neural Radiance Fields ({NeRF}) are a rapidly growing area of research with wide-ranging applications in computer vision, graphics, robotics, and more. In order to streamline the development and deployment of {NeRF} research, we propose a modular {PyTorch} framework, Nerfstudio. Our framework includes plug-and-play components for implementing {NeRF}-based methods, which make it easy for researchers and practitioners to incorporate {NeRF} into their projects. Additionally, the modular design enables support for extensive real-time visualization tools, streamlined pipelines for importing captured in-the-wild data, and tools for exporting to video, point cloud and mesh representations. The modularity of Nerfstudio enables the development of Nerfacto, our method that combines components from recent papers to achieve a balance between speed and quality, while also remaining flexible to future modifications. To promote community-driven development, all associated code and data are made publicly available with open-source licensing at https://nerf.studio.},
	author = {Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brent and Kerr, Justin and Wang, Terrance and Kristoffersen, Alexander and Austin, Jake and Salahi, Kamyar and Ahuja, Abhik and {McAllister}, David and Kanazawa, Angjoo},
	urldate = {2024-09-18},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 4},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Graphics (cs.{GR})},
}

@online{nvidia_nvidia_nodate,
	title = {{NVIDIA} Omniverse for Developers},
	url = {https://developer.nvidia.com/omniverse},
	shorttitle = {{NVIDIA} Omniverse},
	titleaddon = {{NVIDIA} Omniverse for Developers},
	author = {Nvidia},
	urldate = {2024-09-19},
}

@article{la_rocca_opening_2022,
	title = {Opening the Black Box: Bootstrapping Sensitivity Measures in Neural Networks for Interpretable Machine Learning},
	volume = {5},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2571-905X},
	url = {https://www.mdpi.com/2571-905X/5/2/26},
	doi = {10.3390/stats5020026},
	shorttitle = {Opening the Black Box},
	abstract = {Artificial neural networks are powerful tools for data analysis, particularly in the context of highly nonlinear regression models. However, their utility is critically limited due to the lack of interpretation of the model given its black-box nature. To partially address the problem, the paper focuses on the important problem of feature selection. It proposes and discusses a statistical test procedure for selecting a set of input variables that are relevant to the model while taking into account the multiple testing nature of the problem. The approach is within the general framework of sensitivity analysis and uses the conditional expectation of functions of the partial derivatives of the output with respect to the inputs as a sensitivity measure. The proposed procedure extensively uses the bootstrap to approximate the test statistic distribution under the null while controlling the familywise error rate to correct for data snooping arising from multiple testing. In particular, a pair bootstrap scheme was implemented in order to obtain consistent results when using misspecified statistical models, a typical characteristic of neural networks. Numerical examples and a Monte Carlo simulation were carried out to verify the ability of the proposed test procedure to correctly identify the set of relevant features.},
	pages = {440--457},
	number = {2},
	journaltitle = {Stats},
	shortjournal = {Stats},
	author = {La Rocca, Michele and Perna, Cira},
	urldate = {2024-09-20},
	date = {2022-04-25},
	langid = {english},
}

@online{pixar_openusd_nodate,
	title = {{OpenUSD}},
	url = {https://openusd.org/docs/index.html},
	titleaddon = {{OpenUSD}},
	author = {Pixar},
	urldate = {2024-09-19},
}

@article{hahne_real-time_2012,
	title = {Real-time depth imaging},
	rights = {Terms of German Copyright Law},
	url = {https://depositonce.tu-berlin.de/handle/11303/3541},
	doi = {10.14279/DEPOSITONCE-3244},
	abstract = {This thesis depicts approaches toward real-time depth sensing. While humans are very good at estimating distances and hence are able to smoothly control vehicles and their own movements, machines often lack the ability to sense their environment in a manner comparable to humans. This discrepancy prevents the automation of certain job steps. We assume that further enhancement of depth sensing technologies might change this fact. We examine to what extend time-of-flight ({ToF}) cameras are able to provide reliable depth images in real-time. We discuss current issues with existing real-time imaging methods and technologies in detail and present several approaches to enhance real-time depth imaging. We focus on {ToF} imaging and the utilization of {ToF} cameras based on the photonic mixer device ({PMD}) principle. These cameras provide per pixel distance information in real-time. However, the measurement contains several error sources. We present approaches to indicate measurement errors and to determine the reliability of the data from these sensors. If the reliability is known, combining the data with other sensors will become possible. We describe such a combination of {ToF} and stereo cameras that enables new interactive applications in the field of computer graphics. In addition, we show how the fusion of multiple exposures entails improved measurements and extended applications.},
	author = {Hahne, Uwe},
	editora = {{Technische Universität Berlin} and Alexa, Marc},
	editoratype = {collaborator},
	urldate = {2024-09-26},
	date = {2012-06-12},
	langid = {english},
	note = {Publisher: Technische Universität Berlin},
	keywords = {004 Datenverarbeitung; Informatik, Bildfusion, Depth imaging, Image fusion, Sensor fusion, Sensorfusion, Stereo imaging, Stereo-Kamera, Tiefenbildkamera, Time-of-flight Kamera, Time-of-flight camera},
}

@online{noauthor_sick-lidar_nodate,
	title = {Sick-{LiDAR}},
	url = {https://www.sick.com/de/en/produkte-und-loesungen/produkte/lidar-und-radarsensoren/lidar-sensoren/c/g575802},
	titleaddon = {{LiDAR} Sensors},
}

@article{zhou_stereo_2018,
	title = {Stereo magnification: learning view synthesis using multiplane images},
	volume = {37},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3197517.3201323},
	doi = {10.1145/3197517.3201323},
	shorttitle = {Stereo magnification},
	abstract = {The view synthesis problem---generating novel views of a scene from known imagery---has garnered recent attention due in part to compelling applications in virtual and augmented reality. In this paper, we explore an intriguing scenario for view synthesis: extrapolating views from imagery captured by narrow-baseline stereo cameras, including {VR} cameras and now-widespread dual-lens camera phones. We call this problem
              stereo magnification
              , and propose a learning framework that leverages a new layered representation that we call
              multiplane images
              ({MPIs}). Our method also uses a massive new data source for learning view extrapolation: online videos on {YouTube}. Using data mined from such videos, we train a deep network that predicts an {MPI} from an input stereo image pair. This inferred {MPI} can then be used to synthesize a range of novel views of the scene, including views that extrapolate significantly beyond the input baseline. We show that our method compares favorably with several recent view synthesis methods, and demonstrate applications in magnifying narrow-baseline stereo images.},
	pages = {1--12},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Zhou, Tinghui and Tucker, Richard and Flynn, John and Fyffe, Graham and Snavely, Noah},
	urldate = {2024-08-26},
	date = {2018-08-31},
	langid = {english},
}

@article{mayerhofer_bouguerbeerlambert_2020,
	title = {The Bouguer‐Beer‐Lambert Law: Shining Light on the Obscure},
	volume = {21},
	issn = {1439-4235, 1439-7641},
	url = {https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/cphc.202000464},
	doi = {10.1002/cphc.202000464},
	shorttitle = {The Bouguer‐Beer‐Lambert Law},
	abstract = {Abstract
            The Beer‐Lambert law is unquestionably the most important law in optical spectroscopy and indispensable for the qualitative and quantitative interpretation of spectroscopic data. As such, every spectroscopist should know its limits and potential pitfalls, arising from its application, by heart. It is the goal of this work to review these limits and pitfalls, as well as to provide solutions and explanations to guide the reader. This guidance will allow a deeper understanding of spectral features, which cannot be explained by the Beer‐Lambert law, because they arise from electromagnetic effects/the wave nature of light. Those features include band shifts and intensity changes based exclusively upon optical conditions, i. e. the method chosen to record the spectra, the substrate and the form of the sample. As such, the review will be an essential tool towards a full understanding of optical spectra and their quantitative interpretation based not only on oscillator positions, but also on their strengths and damping constants.},
	pages = {2029--2046},
	number = {18},
	journaltitle = {{ChemPhysChem}},
	shortjournal = {{ChemPhysChem}},
	author = {Mayerhöfer, Thomas G. and Pahlow, Susanne and Popp, Jürgen},
	urldate = {2024-10-10},
	date = {2020-09-15},
	langid = {english},
}

@article{rudin_why_2019,
	title = {Why Are We Using Black Box Models in {AI} When We Don’t Need To? A Lesson From An Explainable {AI} Competition},
	volume = {1},
	url = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8},
	doi = {10.1162/99608f92.5a8a3a3d},
	shorttitle = {Why Are We Using Black Box Models in {AI} When We Don’t Need To?},
	number = {2},
	journaltitle = {Harvard Data Science Review},
	shortjournal = {Harvard Data Science Review},
	author = {Rudin, Cynthia and Radin, Joanna},
	urldate = {2024-09-27},
	date = {2019-11-01},
	langid = {english},
}
