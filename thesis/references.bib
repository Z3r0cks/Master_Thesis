@misc{mcdermott_probabilistic_2024,
	title = {A Probabilistic Formulation of {LiDAR} Mapping with Neural Radiance Fields},
	url = {http://arxiv.org/abs/2411.01725},
	abstract = {In this paper we reexamine the process through which a Neural Radiance Field ({NeRF}) can be trained to produce novel {LiDAR} views of a scene. Unlike image applications where camera pixels integrate light over time, {LiDAR} pulses arrive at specific times. As such, multiple {LiDAR} returns are possible for any given detector and the classification of these returns is inherently probabilistic. Applying a traditional {NeRF} training routine can result in the network learning phantom surfaces in free space between conflicting range measurements, similar to how floater aberrations may be produced by an image model. We show that by formulating loss as an integral of probability (rather than as an integral of optical density) the network can learn multiple peaks for a given ray, allowing the sampling of first, nth, or strongest returns from a single output channel. Code is available at https://github.com/mcdermatt/{PLINK}},
	number = {{arXiv}:2411.01725},
	publisher = {{arXiv}},
	author = {{McDermott}, Matthew and Rife, Jason},
	urldate = {2024-11-08},
	date = {2024-11-03},
	eprinttype = {arxiv},
	eprint = {2411.01725 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, I.4.5},
}

@inproceedings{schleise_automated_2024,
	title = {An automated Pipeline to bring {NeRFs} to the Industrial Metaverse},
	url = {http://wscg.zcu.cz/WSCG2024/CSRN-2024/B59-2024.pdf},
	doi = {10.24132/CSRN.3401.17},
	abstract = {The Industrial Metaverse offers new opportunities in various industrial sectors, such as training, product development, and collaboration. It relies on rendering 3D images of industrial plants, factories, and machines that are viewed by multiple human observers in various applications. Creating these 3D scenes requires either elaborate 3D modeling efforts or complex 3D reconstruction methods. Both methods require expensive hardware and highly trained individuals to create photo-realistic results. To overcome these limitations, we offer an end-to-end pipeline that generates photo-realistic 3D scene renderings from video input using {NeRF} (neural radiance fields). Our tool automates the entire process, resulting in remarkable efficiency gains. It reduces the creation time from days to minutes, even for untrained users.},
	eventtitle = {Computer Science Research Notes},
	author = {Schleise, Sabine and Hahne, Uwe and {Hochschule Furtwangen} and Lindinger, Jakob and {SICK AG}},
	urldate = {2024-09-26},
	date = {2024},
}

@article{li_efficient_2024,
	title = {An efficient deep learning-based framework for image distortion correction},
	volume = {40},
	issn = {0178-2789, 1432-2315},
	url = {https://link.springer.com/10.1007/s00371-024-03580-3},
	doi = {10.1007/s00371-024-03580-3},
	pages = {6955--6967},
	number = {10},
	journaltitle = {The Visual Computer},
	shortjournal = {Vis Comput},
	author = {Li, Sicheng and Chu, Yuhui and Zhao, Yunpeng and Zhao, Pengpeng},
	urldate = {2024-11-07},
	date = {2024-10},
	langid = {english},
}

@article{arribas_indirect_2020,
	title = {An indirect measurement of the speed of light in a General Physics Laboratory},
	volume = {32},
	issn = {10183647},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S101836472030210X},
	doi = {10.1016/j.jksus.2020.06.017},
	pages = {2797--2802},
	number = {6},
	journaltitle = {Journal of King Saud University - Science},
	shortjournal = {Journal of King Saud University - Science},
	author = {Arribas, Enrique and Escobar, Isabel and Ramirez-Vazquez, Raquel and Franco, Teresa and Belendez, Augusto},
	urldate = {2024-09-27},
	date = {2020-09},
	langid = {english},
}

@misc{loetscher_assessing_2023,
	title = {Assessing the Robustness of {LiDAR}, Radar and Depth Cameras Against Ill-Reflecting Surfaces in Autonomous Vehicles: An Experimental Study},
	url = {http://arxiv.org/abs/2309.10504},
	shorttitle = {Assessing the Robustness of {LiDAR}, Radar and Depth Cameras Against Ill-Reflecting Surfaces in Autonomous Vehicles},
	abstract = {Range-measuring sensors play a critical role in autonomous driving systems. While {LiDAR} technology has been dominant, its vulnerability to adverse weather conditions is well-documented. This paper focuses on secondary adverse conditions and the implications of ill-reflective surfaces on range measurement sensors. We assess the influence of this condition on the three primary ranging modalities used in autonomous mobile robotics: {LiDAR}, {RADAR}, and Depth-Camera. Based on accurate experimental evaluation the papers findings reveal that under ill-reflectivity, {LiDAR} ranging performance drops significantly to 33\% of its nominal operating conditions, whereas {RADAR} and Depth-Cameras maintain up to 100\% of their nominal distance ranging capabilities. Additionally, we demonstrate on a 1:10 scaled autonomous racecar how ill-reflectivity adversely impacts downstream robotics tasks, highlighting the necessity for robust range sensing in autonomous driving.},
	number = {{arXiv}:2309.10504},
	publisher = {{arXiv}},
	author = {Loetscher, Michael and Baumann, Nicolas and Ghignone, Edoardo and Ronco, Andrea and Magno, Michele},
	urldate = {2024-11-09},
	date = {2023-09-19},
	eprinttype = {arxiv},
	eprint = {2309.10504 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2024-09-02},
	date = {2015-05-28},
	langid = {english},
}

@inproceedings{dehong_design_2017,
	location = {Chongqing, China},
	title = {Design and implementation of {LiDAR} navigation system based on triangulation measurement},
	url = {http://ieeexplore.ieee.org/document/7978258/},
	doi = {10.1109/ccdc.2017.7978258},
	eventtitle = {2017 29th Chinese Control And Decision Conference ({CCDC})},
	pages = {6060--6063},
	booktitle = {2017 29th Chinese Control And Decision Conference ({CCDC})},
	publisher = {{IEEE}},
	author = {Dehong, Cong and Liangqi, Zhang and Pengpeng, Su and Zaiyang, Tang and Yuhao, Meng and Yong, Wang},
	urldate = {2024-09-17},
	date = {2017-05},
}

@misc{niemeyer_differentiable_2020,
	title = {Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision},
	url = {http://arxiv.org/abs/1912.07372},
	shorttitle = {Differentiable Volumetric Rendering},
	abstract = {Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from {RGB} images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from {RGB} images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.},
	number = {{arXiv}:1912.07372},
	publisher = {{arXiv}},
	author = {Niemeyer, Michael and Mescheder, Lars and Oechsle, Michael and Geiger, Andreas},
	urldate = {2024-09-04},
	date = {2020-03-23},
	eprinttype = {arxiv},
	eprint = {1912.07372 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{muller_instant_2022,
	title = {Instant neural graphics primitives with a multiresolution hash encoding},
	volume = {41},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3528223.3530127},
	doi = {10.1145/3528223.3530127},
	abstract = {Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern {GPUs}. We leverage this parallelism by implementing the whole system using fully-fused {CUDA} kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080.},
	pages = {1--15},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Müller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
	urldate = {2024-09-18},
	date = {2022-07},
	langid = {english},
}

@inproceedings{laconte_lidar_2019,
	title = {Lidar Measurement Bias Estimation via Return Waveform Modelling in a Context of 3D Mapping},
	url = {http://arxiv.org/abs/1810.01619},
	doi = {10.1109/ICRA.2019.8793671},
	abstract = {In a context of 3D mapping, it is very important to get accurate measurements from sensors. In particular, Light Detection And Ranging ({LIDAR}) measurements are typically treated as a zero-mean Gaussian distribution. We show that this assumption leads to predictable localisation drifts, especially when a bias related to measuring obstacles with high incidence angles is not taken into consideration. Moreover, we present a way to physically understand and model this bias, which generalises to multiple sensors. Using an experimental setup, we measured the bias of the Sick {LMS}151, Velodyne {HDL}-32E, and Robosense {RS}-{LiDAR}-16 as a function of depth and incidence angle, and showed that the bias can go up to 20 cm for high incidence angles. We then used our modelisations to remove the bias from the measurements, leading to more accurate maps and a reduced localisation drift.},
	pages = {8100--8106},
	booktitle = {2019 International Conference on Robotics and Automation ({ICRA})},
	author = {Laconte, Johann and Deschênes, Simon-Pierre and Labussière, Mathieu and Pomerleau, François},
	urldate = {2024-11-09},
	date = {2019-05},
	eprinttype = {arxiv},
	eprint = {1810.01619 [cs]},
	keywords = {68T40, Computer Science - Robotics},
}

@misc{barron_mip-nerf_2021,
	title = {Mip-{NeRF}: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2103.13415},
	doi = {10.48550/ARXIV.2103.13415},
	shorttitle = {Mip-{NeRF}},
	abstract = {The rendering procedure used by neural radiance fields ({NeRF}) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for {NeRF}, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-{NeRF}" (a la "mipmap"), extends {NeRF} to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-{NeRF} reduces objectionable aliasing artifacts and significantly improves {NeRF}'s ability to represent fine details, while also being 7\% faster than {NeRF} and half the size. Compared to {NeRF}, mip-{NeRF} reduces average error rates by 17\% on the dataset presented with {NeRF} and by 60\% on a challenging multiscale variant of that dataset that we present. Mip-{NeRF} is also able to match the accuracy of a brute-force supersampled {NeRF} on our multiscale dataset while being 22x faster.},
	publisher = {{arXiv}},
	author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and Martin-Brualla, Ricardo and Srinivasan, Pratul P.},
	urldate = {2024-09-18},
	date = {2021},
	note = {Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Graphics (cs.{GR})},
}

@misc{zhang_nerf-lidar_2024,
	title = {{NeRF}-{LiDAR}: Generating Realistic {LiDAR} Point Clouds with Neural Radiance Fields},
	url = {http://arxiv.org/abs/2304.14811},
	shorttitle = {{NeRF}-{LiDAR}},
	abstract = {Labeling {LiDAR} point clouds for training autonomous driving is extremely expensive and difficult. {LiDAR} simulation aims at generating realistic {LiDAR} data with labels for training and verifying self-driving algorithms more efficiently. Recently, Neural Radiance Fields ({NeRF}) have been proposed for novel view synthesis using implicit reconstruction of 3D scenes. Inspired by this, we present {NeRF}-{LIDAR}, a novel {LiDAR} simulation method that leverages real-world information to generate realistic {LIDAR} point clouds. Different from existing {LiDAR} simulators, we use real images and point cloud data collected by self-driving cars to learn the 3D scene representation, point cloud generation and label rendering. We verify the effectiveness of our {NeRF}-{LiDAR} by training different 3D segmentation models on the generated {LiDAR} point clouds. It reveals that the trained models are able to achieve similar accuracy when compared with the same model trained on the real {LiDAR} data. Besides, the generated data is capable of boosting the accuracy through pre-training which helps reduce the requirements of the real labeled data.},
	number = {{arXiv}:2304.14811},
	publisher = {{arXiv}},
	author = {Zhang, Junge and Zhang, Feihu and Kuang, Shaochen and Zhang, Li},
	urldate = {2024-09-19},
	date = {2024-01-20},
	eprinttype = {arxiv},
	eprint = {2304.14811 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{deng_nerf-loam_2023,
	title = {{NeRF}-{LOAM}: Neural Implicit Representation for Large-Scale Incremental {LiDAR} Odometry and Mapping},
	url = {http://arxiv.org/abs/2303.10709},
	shorttitle = {{NeRF}-{LOAM}},
	abstract = {Simultaneously odometry and mapping using {LiDAR} data is an important task for mobile systems to achieve full autonomy in large-scale environments. However, most existing {LiDAR}-based methods prioritize tracking quality over reconstruction quality. Although the recently developed neural radiance fields ({NeRF}) have shown promising advances in implicit reconstruction for indoor environments, the problem of simultaneous odometry and mapping for large-scale scenarios using incremental {LiDAR} data remains unexplored. To bridge this gap, in this paper, we propose a novel {NeRF}-based {LiDAR} odometry and mapping approach, {NeRF}-{LOAM}, consisting of three modules neural odometry, neural mapping, and mesh reconstruction. All these modules utilize our proposed neural signed distance function, which separates {LiDAR} points into ground and non-ground points to reduce Z-axis drift, optimizes odometry and voxel embeddings concurrently, and in the end generates dense smooth mesh maps of the environment. Moreover, this joint optimization allows our {NeRF}-{LOAM} to be pre-trained free and exhibit strong generalization abilities when applied to different environments. Extensive evaluations on three publicly available datasets demonstrate that our approach achieves state-of-the-art odometry and mapping performance, as well as a strong generalization in large-scale environments utilizing {LiDAR} data. Furthermore, we perform multiple ablation studies to validate the effectiveness of our network design. The implementation of our approach will be made available at https://github.com/{JunyuanDeng}/{NeRF}-{LOAM}.},
	number = {{arXiv}:2303.10709},
	publisher = {{arXiv}},
	author = {Deng, Junyuan and Chen, Xieyuanli and Xia, Songpengcheng and Sun, Zhen and Liu, Guoqing and Yu, Wenxian and Pei, Ling},
	urldate = {2024-11-08},
	date = {2023-03-19},
	eprinttype = {arxiv},
	eprint = {2303.10709 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{mildenhall_nerf_2020,
	title = {{NeRF}: Representing Scenes as Neural Radiance Fields for View Synthesis},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2003.08934},
	doi = {10.48550/ARXIV.2003.08934},
	shorttitle = {{NeRF}},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(θ, ϕ)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	publisher = {{arXiv}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	urldate = {2024-07-18},
	date = {2020},
	note = {Version Number: 2},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Graphics (cs.{GR})},
}

@article{tancik_nerfstudio_2023,
	title = {Nerfstudio: A Modular Framework for Neural Radiance Field Development},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2302.04264},
	doi = {10.48550/ARXIV.2302.04264},
	shorttitle = {Nerfstudio},
	abstract = {Neural Radiance Fields ({NeRF}) are a rapidly growing area of research with wide-ranging applications in computer vision, graphics, robotics, and more. In order to streamline the development and deployment of {NeRF} research, we propose a modular {PyTorch} framework, Nerfstudio. Our framework includes plug-and-play components for implementing {NeRF}-based methods, which make it easy for researchers and practitioners to incorporate {NeRF} into their projects. Additionally, the modular design enables support for extensive real-time visualization tools, streamlined pipelines for importing captured in-the-wild data, and tools for exporting to video, point cloud and mesh representations. The modularity of Nerfstudio enables the development of Nerfacto, our method that combines components from recent papers to achieve a balance between speed and quality, while also remaining flexible to future modifications. To promote community-driven development, all associated code and data are made publicly available with open-source licensing at https://nerf.studio.},
	author = {Tancik, Matthew and Weber, Ethan and Ng, Evonne and Li, Ruilong and Yi, Brent and Kerr, Justin and Wang, Terrance and Kristoffersen, Alexander and Austin, Jake and Salahi, Kamyar and Ahuja, Abhik and {McAllister}, David and Kanazawa, Angjoo},
	urldate = {2024-09-18},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 4},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Graphics (cs.{GR})},
}

@online{black_nikonusa_nodate,
	title = {nikonusa},
	url = {https://www.nikonusa.com/learn-and-explore/c/tips-and-techniques/understanding-focal-length},
	titleaddon = {Understanding Focal Length},
	author = {Black, Dave and Berkenfeld, Diane and Silverman, Lindsay},
}

@online{nvidia_nvidia_nodate,
	title = {{NVIDIA} Omniverse for Developers},
	url = {https://developer.nvidia.com/omniverse},
	shorttitle = {{NVIDIA} Omniverse},
	titleaddon = {{NVIDIA} Omniverse for Developers},
	author = {Nvidia},
	urldate = {2024-09-19},
}

@article{la_rocca_opening_2022,
	title = {Opening the Black Box: Bootstrapping Sensitivity Measures in Neural Networks for Interpretable Machine Learning},
	volume = {5},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2571-905X},
	url = {https://www.mdpi.com/2571-905X/5/2/26},
	doi = {10.3390/stats5020026},
	shorttitle = {Opening the Black Box},
	abstract = {Artificial neural networks are powerful tools for data analysis, particularly in the context of highly nonlinear regression models. However, their utility is critically limited due to the lack of interpretation of the model given its black-box nature. To partially address the problem, the paper focuses on the important problem of feature selection. It proposes and discusses a statistical test procedure for selecting a set of input variables that are relevant to the model while taking into account the multiple testing nature of the problem. The approach is within the general framework of sensitivity analysis and uses the conditional expectation of functions of the partial derivatives of the output with respect to the inputs as a sensitivity measure. The proposed procedure extensively uses the bootstrap to approximate the test statistic distribution under the null while controlling the familywise error rate to correct for data snooping arising from multiple testing. In particular, a pair bootstrap scheme was implemented in order to obtain consistent results when using misspecified statistical models, a typical characteristic of neural networks. Numerical examples and a Monte Carlo simulation were carried out to verify the ability of the proposed test procedure to correctly identify the set of relevant features.},
	pages = {440--457},
	number = {2},
	journaltitle = {Stats},
	shortjournal = {Stats},
	author = {La Rocca, Michele and Perna, Cira},
	urldate = {2024-09-20},
	date = {2022-04-25},
	langid = {english},
}

@online{pixar_openusd_nodate,
	title = {{OpenUSD}},
	url = {https://openusd.org/docs/index.html},
	titleaddon = {{OpenUSD}},
	author = {Pixar},
	urldate = {2024-09-19},
}

@article{pan_pin-slam_2024,
	title = {{PIN}-{SLAM}: {LiDAR} {SLAM} Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency},
	volume = {40},
	issn = {1552-3098, 1941-0468},
	url = {http://arxiv.org/abs/2401.09101},
	doi = {10.1109/TRO.2024.3422055},
	shorttitle = {{PIN}-{SLAM}},
	abstract = {Accurate and robust localization and mapping are essential components for most autonomous robots. In this paper, we propose a {SLAM} system for building globally consistent maps, called {PIN}-{SLAM}, that is based on an elastic and compact point-based implicit neural map representation. Taking range measurements as input, our approach alternates between incremental learning of the local implicit signed distance field and the pose estimation given the current local map using a correspondence-free, point-to-implicit model registration. Our implicit map is based on sparse optimizable neural points, which are inherently elastic and deformable with the global pose adjustment when closing a loop. Loops are also detected using the neural point features. Extensive experiments validate that {PIN}-{SLAM} is robust to various environments and versatile to different range sensors such as {LiDAR} and {RGB}-D cameras. {PIN}-{SLAM} achieves pose estimation accuracy better or on par with the state-of-the-art {LiDAR} odometry or {SLAM} systems and outperforms the recent neural implicit {SLAM} approaches while maintaining a more consistent, and highly compact implicit map that can be reconstructed as accurate and complete meshes. Finally, thanks to the voxel hashing for efficient neural points indexing and the fast implicit map-based registration without closest point association, {PIN}-{SLAM} can run at the sensor frame rate on a moderate {GPU}. Codes will be available at: https://github.com/{PRBonn}/{PIN}\_SLAM.},
	pages = {4045--4064},
	journaltitle = {{IEEE} Transactions on Robotics},
	shortjournal = {{IEEE} Trans. Robot.},
	author = {Pan, Yue and Zhong, Xingguang and Wiesmann, Louis and Posewsky, Thorbjörn and Behley, Jens and Stachniss, Cyrill},
	urldate = {2024-11-08},
	date = {2024},
	eprinttype = {arxiv},
	eprint = {2401.09101 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{hahne_real-time_2012,
	title = {Real-time depth imaging},
	rights = {Terms of German Copyright Law},
	url = {https://depositonce.tu-berlin.de/handle/11303/3541},
	doi = {10.14279/DEPOSITONCE-3244},
	abstract = {This thesis depicts approaches toward real-time depth sensing. While humans are very good at estimating distances and hence are able to smoothly control vehicles and their own movements, machines often lack the ability to sense their environment in a manner comparable to humans. This discrepancy prevents the automation of certain job steps. We assume that further enhancement of depth sensing technologies might change this fact. We examine to what extend time-of-flight ({ToF}) cameras are able to provide reliable depth images in real-time. We discuss current issues with existing real-time imaging methods and technologies in detail and present several approaches to enhance real-time depth imaging. We focus on {ToF} imaging and the utilization of {ToF} cameras based on the photonic mixer device ({PMD}) principle. These cameras provide per pixel distance information in real-time. However, the measurement contains several error sources. We present approaches to indicate measurement errors and to determine the reliability of the data from these sensors. If the reliability is known, combining the data with other sensors will become possible. We describe such a combination of {ToF} and stereo cameras that enables new interactive applications in the field of computer graphics. In addition, we show how the fusion of multiple exposures entails improved measurements and extended applications.},
	author = {Hahne, Uwe},
	editora = {{Technische Universität Berlin} and Alexa, Marc},
	editoratype = {collaborator},
	urldate = {2024-09-26},
	date = {2012-06-12},
	langid = {english},
	note = {Publisher: Technische Universität Berlin},
	keywords = {004 Datenverarbeitung; Informatik, Bildfusion, Depth imaging, Image fusion, Sensor fusion, Sensorfusion, Stereo imaging, Stereo-Kamera, Tiefenbildkamera, Time-of-flight Kamera, Time-of-flight camera},
}

@inproceedings{zhong_shine-mapping_2023,
	location = {London, United Kingdom},
	title = {{SHINE}-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350323658},
	url = {https://ieeexplore.ieee.org/document/10160907/},
	doi = {10.1109/ICRA48891.2023.10160907},
	shorttitle = {{SHINE}-Mapping},
	eventtitle = {2023 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {8371--8377},
	booktitle = {2023 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	publisher = {{IEEE}},
	author = {Zhong, Xingguang and Pan, Yue and Behley, Jens and Stachniss, Cyrill},
	urldate = {2024-11-08},
	date = {2023-05-29},
}

@online{noauthor_sick-lidar_nodate,
	title = {Sick-{LiDAR}},
	url = {https://www.sick.com/de/en/produkte-und-loesungen/produkte/lidar-und-radarsensoren/lidar-sensoren/c/g575802},
	titleaddon = {{LiDAR} Sensors},
}

@article{zhou_stereo_2018,
	title = {Stereo magnification: learning view synthesis using multiplane images},
	volume = {37},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3197517.3201323},
	doi = {10.1145/3197517.3201323},
	shorttitle = {Stereo magnification},
	abstract = {The view synthesis problem---generating novel views of a scene from known imagery---has garnered recent attention due in part to compelling applications in virtual and augmented reality. In this paper, we explore an intriguing scenario for view synthesis: extrapolating views from imagery captured by narrow-baseline stereo cameras, including {VR} cameras and now-widespread dual-lens camera phones. We call this problem
              stereo magnification
              , and propose a learning framework that leverages a new layered representation that we call
              multiplane images
              ({MPIs}). Our method also uses a massive new data source for learning view extrapolation: online videos on {YouTube}. Using data mined from such videos, we train a deep network that predicts an {MPI} from an input stereo image pair. This inferred {MPI} can then be used to synthesize a range of novel views of the scene, including views that extrapolate significantly beyond the input baseline. We show that our method compares favorably with several recent view synthesis methods, and demonstrate applications in magnifying narrow-baseline stereo images.},
	pages = {1--12},
	number = {4},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Zhou, Tinghui and Tucker, Richard and Flynn, John and Fyffe, Graham and Snavely, Noah},
	urldate = {2024-08-26},
	date = {2018-08-31},
	langid = {english},
}

@article{mayerhofer_bouguerbeerlambert_2020,
	title = {The Bouguer‐Beer‐Lambert Law: Shining Light on the Obscure},
	volume = {21},
	issn = {1439-4235, 1439-7641},
	url = {https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/cphc.202000464},
	doi = {10.1002/cphc.202000464},
	shorttitle = {The Bouguer‐Beer‐Lambert Law},
	abstract = {Abstract
            The Beer‐Lambert law is unquestionably the most important law in optical spectroscopy and indispensable for the qualitative and quantitative interpretation of spectroscopic data. As such, every spectroscopist should know its limits and potential pitfalls, arising from its application, by heart. It is the goal of this work to review these limits and pitfalls, as well as to provide solutions and explanations to guide the reader. This guidance will allow a deeper understanding of spectral features, which cannot be explained by the Beer‐Lambert law, because they arise from electromagnetic effects/the wave nature of light. Those features include band shifts and intensity changes based exclusively upon optical conditions, i. e. the method chosen to record the spectra, the substrate and the form of the sample. As such, the review will be an essential tool towards a full understanding of optical spectra and their quantitative interpretation based not only on oscillator positions, but also on their strengths and damping constants.},
	pages = {2029--2046},
	number = {18},
	journaltitle = {{ChemPhysChem}},
	shortjournal = {{ChemPhysChem}},
	author = {Mayerhöfer, Thomas G. and Pahlow, Susanne and Popp, Jürgen},
	urldate = {2024-10-10},
	date = {2020-09-15},
	langid = {english},
}

@article{klaas-witt_five_2022,
	title = {The five main influencing factors for lidar errors in complex terrain},
	volume = {7},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2366-7451},
	url = {https://wes.copernicus.org/articles/7/413/2022/},
	doi = {10.5194/wes-7-413-2022},
	abstract = {Abstract. Lidars have become a valuable technology to assess the wind resource at hub
height of modern wind turbines. However, because of the assumption of
homogeneous flow in their wind vector reconstruction algorithms, common wind profile Doppler lidars suffer from errors at complex terrain sites. This study analyses the impact of the five main influencing factors for lidar measurement errors in complex terrain, i.e. orographic complexity,
measurement height, surface roughness and forest, atmospheric stability, and half-cone opening angle, in a non-dimensional, model-based parameter study. In a novel approach, the lidar error ε is split up into a part εc, caused by flow curvature at the measurement points of the lidar, and a part εs, caused by the local speed-up effects between the measurement points. This approach allows for a systematic and complete interpretation of the influence of the half-cone opening angle φ of the lidar on the total lidar error ε. It also provides information about the uncertainty in simple lidar error estimations that are based on inflow and outflow angles at the measurement points. The model-based parameter study is limited to two-dimensional Gaussian hills with hill height H and hill half-width L. H/L and z/L, with z being the measurement height, are identified as the main scaling factors for the lidar error. Three flow models of different complexity are used to estimate the lidar errors. The outcome of the study provides various findings that enable an assessment of the applicability of these flow models. The study clearly shows that orographic complexity, roughness and forest
characteristics, and atmospheric stability have a significant
influence on lidar error estimation. Based on the error separation approach
it furthermore allows for an in-depth analysis of the influence of reduced
half-cone opening angles, explaining contradiction in the previously
available literature. The choice and parameterization of flow models and the design of methods for lidar error estimation are found to be essential to achieve accurate results. The use of a Reynolds-averaged Navier–Stokes ({RANS}) computational fluid dynamics ({CFD}) model in conjunction with an appropriate forest model is highly recommended for lidar error estimations in complex terrain since forest (and roughness) tends to reduce the lidar error. If atmospheric stability variation at a measurement site plays a vital role, it should also be considered in the modelling. When planning a measurement campaign, an accurate estimation of the predicted lidar error should be carried out in advance to choose a reasonable measurement location. This will decrease measurement uncertainties and maximize the value of the measurement data.},
	pages = {413--431},
	number = {1},
	journaltitle = {Wind Energy Science},
	shortjournal = {Wind Energ. Sci.},
	author = {Klaas-Witt, Tobias and Emeis, Stefan},
	urldate = {2024-11-09},
	date = {2022-03-01},
	langid = {english},
}

@article{clarke_principal_1998,
	title = {The Principal Point and {CCD} Cameras},
	volume = {16},
	issn = {0031-868X, 1477-9730},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/0031-868X.00127},
	doi = {10.1111/0031-868X.00127},
	abstract = {The principal point has long been regarded as one of the fundamental parameters in camera calibration. In the age of film based aerial and large format terrestrial cameras, the principal point could be located by a variety of techniques with a certainty of ±10 mm (Carman and Brown, 1961) and this was considered sufficient. However, aerial cameras were precision, purpose built, expensive pieces of equipment where the assembly was painstaking and the location of the principal point measured to a known tolerance. In the digital era, photogrammetrists, and many others, are using cameras which have not been specifically designed or built for photogrammetry. For these cameras there is no requirement for the manufacturers to position the lens in a pre‐defined location relative to the image sensing plane or for the lens manufacturer to align the lens elements precisely. In fact, deviations from the centre of the sensor can be a considerable percentage of the extent of the sensor (up to 10 per cent for some zoom lenses (Burner, 1995)). This paper discusses the development of methods of obtaining the location of the principal point, considers the relationship between the principal point and other parameters in the functional model, and shows how the location of this point can be estimated with and without recourse to autocollimation methods.},
	pages = {293--312},
	number = {92},
	journaltitle = {The Photogrammetric Record},
	shortjournal = {The Photogrammetric Record},
	author = {Clarke, T. A. and Wang, X. and Fryer, J. G.},
	urldate = {2024-11-07},
	date = {1998-10},
	langid = {english},
}

@article{rudin_why_2019,
	title = {Why Are We Using Black Box Models in {AI} When We Don’t Need To? A Lesson From An Explainable {AI} Competition},
	volume = {1},
	url = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8},
	doi = {10.1162/99608f92.5a8a3a3d},
	shorttitle = {Why Are We Using Black Box Models in {AI} When We Don’t Need To?},
	number = {2},
	journaltitle = {Harvard Data Science Review},
	shortjournal = {Harvard Data Science Review},
	author = {Rudin, Cynthia and Radin, Joanna},
	urldate = {2024-09-27},
	date = {2019-11-01},
	langid = {english},
}
